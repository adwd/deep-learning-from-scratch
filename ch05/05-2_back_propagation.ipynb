{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤの実装\n",
    "\n",
    "### ReLUレイヤ\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = \\left \\{\n",
    "\\begin{array}{l}\n",
    "x (x \\gt 0) \\\\\n",
    "0 (x \\leqq 0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial y}{\\partial x} = \\left \\{\n",
    "\\begin{array}{l}\n",
    "1 (x \\gt 0) \\\\\n",
    "0 (x \\leqq 0)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None # 順伝搬の入力で0以上のものを覚えておく\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoidレイヤ\n",
    "\n",
    "$$y = \\frac{1}{1 + exp(-x)}$$\n",
    "\n",
    "乗算、加算ノードの他にexp、除算ノードが必要。\n",
    "\n",
    "除算ノードは$y = \\frac{1}{x}$で、$\\partial y / \\partial x = - 1 / x^2 = - y^2$となる。\n",
    "\n",
    "expノードは$y = exp(x)$を表し、微分しても同じになる。\n",
    "\n",
    "Sigmoidの各項の逆伝搬を計算すると、$\\partial L / \\partial y \\cdot y^2 exp(-x)$となる。\n",
    "\n",
    "この逆伝搬の式が入力のxと出力のyのみから表されるため、局所計算のノードのグループではなくsigmoidノード一つにまとめることができる。\n",
    "\n",
    "さらに、$\\partial L / \\partial y \\cdot y^2 exp(-x) = \\partial L / \\partial y \\cdot y(1-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine/Softmaxレイヤの実装\n",
    "\n",
    "### Affineレイヤ\n",
    "\n",
    "ニューラルネットワークの順伝搬での重み付き信号の総和を計算するのに、行列の内積を用いた。\n",
    "幾何学の分野で行列の内積はアフィン変換と呼ばれるため、行列の内積を行う処理をAffineレイヤという名前にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2,), (2, 3), (3,), array([ 1.3605097 ,  1.51454222,  1.72421504]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "X.shape, W.shape, B.shape, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affineレイヤも計算グラフを用いて内積とバイアスの和に計算グラフを局所化し、逆伝搬を求める。\n",
    "\n",
    "これまでのレイヤはスカラ値が信号だったが、ここでは行列がノード間を伝搬する。\n",
    "\n",
    "内積部分の逆伝搬は以下の式で表される。\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ版Affineレイヤ\n",
    "\n",
    "N個のデータをまとめて順伝搬するAffineレイヤを考える。X, X dot W, Yの次元が異なるだけで、計算グラフは同様となる。\n",
    "\n",
    "逆伝搬の計算も同様に行うことができるが、バイアスの加算に関してはそれぞれのデータの逆伝搬の値がバイアスの要素に集約されるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax-with-Lossレイヤ\n",
    "\n",
    "> Softmax関数\n",
    "> ニューラルネットワークで分類問題を扱う際に出力層の活性化関数に用いる。\n",
    "> 各項の総和が1になるので、そのまま確率として扱うことができる。\n",
    "> $$y_k = \\frac{exp(a_k)}{\\sum_{i = 1}^n exp(a_i)}$$\n",
    "\n",
    "> 損失関数\n",
    "> 学習の指標となる関数。2乗和誤差や交差エントロピー誤差がよく用いられる。\n",
    "> 損失関数を設定するとパラメータの勾配を計算できるようになるため用いられる。\n",
    "\n",
    "> 2乗和誤差\n",
    "> $$E = \\frac{1}{2} \\sum_k (y_k - t_k)^2$$\n",
    "> 交差エントロピー誤差\n",
    "> $$E = - \\sum_k t_k \\log y_k $$\n",
    "\n",
    "Softmaxレイヤに合わせて損失関数として交差エントロピー誤差も含めて実装する。\n",
    "\n",
    "この逆伝搬を計算すると、 $y_i - t-i$というキレイな結果になる。Softmaxレイヤの出力と教師ラベルの差分になる。\n",
    "\n",
    "ニューラルネットワークの学習の目的は、ニューラルネットワークの出力（Softmaxの出力）を教師ラベルに近づけることなので、この逆伝搬の結果は現在のニューラルネットワークと教師ラベルの誤差を素直に表している。\n",
    "\n",
    "> これは偶然ではなく、逆伝搬がきれいになるように設計されたのが交差エントロピー誤差である。回帰問題において2乗和誤差を恒等関数の損失関数に用いた場合も同じ逆伝搬となり、これも同様の意図がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 損失\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ (one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
